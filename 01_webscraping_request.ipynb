{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Webscraping Request\n",
    "\n",
    "#### Hent data og dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "workingProxyList = [] # used for proxies (free ones are too slow)\n",
    "totalpageprShop  = [] # used in multiThreadGetTotalPages\n",
    "allPagesRequest  = [] # used in multiThreadDownloader\n",
    "reviewErrorCount = [] # used in multithreadwritecsvfile\n",
    "dataDir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denne celle skal køres, hvis der skal bruges proxies\n",
    "\n",
    "#with open('ipList2', 'rb') as file:\n",
    "#    workingProxyList = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitter hver url i listen, og returnere hjemmesidenavnet.\n",
    "def createFiles(url):\n",
    "    filename = str(url.url)\n",
    "    filename = filename.split('.')\n",
    "\n",
    "    if len(filename) == 4:\n",
    "        filename = filename[2].split('/')\n",
    "        filename = filename[2]\n",
    "    else:\n",
    "        filename = filename[3]\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Henter antal review sider for hver butik i listen og gemmer det som en liste af tuples.\n",
    "def multiThreadGetTotalPages(shop):\n",
    "    shop = 'https://dk.trustpilot.com/review/'+shop\n",
    "    progress.update(1)\n",
    "    listofpages=\"\"\n",
    "    totalpages=None\n",
    "    req = requests.get(shop, timeout=10)\n",
    "    soup = bs4.BeautifulSoup(req.content, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        totalpages = soup.find('a', attrs={'name':'pagination-button-last'}).text\n",
    "        shopAndPage = {'shop': shop,'pages': totalpages}\n",
    "        totalpageprShop.append(shopAndPage)\n",
    "    except: \n",
    "        listofpages = [7,6,5,4,3,2,1]\n",
    "        for content in listofpages:\n",
    "            try:\n",
    "                totalpages = soup.find('a', attrs={'name':'pagination-button-'+str(content)}).text\n",
    "                if totalpages is not None:\n",
    "                    shopAndPage = {'shop': shop,'pages': totalpages}\n",
    "                    totalpageprShop.append(shopAndPage)\n",
    "                    break\n",
    "                \n",
    "            except Exception as e:\n",
    "                pass\n",
    "           \n",
    "    if len(totalpageprShop) == 0:\n",
    "        shopAndPage = {'shop': shop,'pages': 1}\n",
    "        totalpageprShop.append(shopAndPage)\n",
    "    \n",
    "    print(shopAndPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Henter alle siderne fra trustpilot og tilføjet dem i en liste. Før noget filtering.\n",
    "def multiThreadDownloader(shop,page):\n",
    "    progress.update(1)\n",
    "    page = page+1\n",
    "    url = shop+'?page='+str(page)\n",
    "    \n",
    "    #Overvej at droppe proxy.. Servers er langsomme, timer ofte ud eller virker slet ikke. \n",
    "    #Selvom de er testet 5min forinden.\n",
    "    if len(workingProxyList) > 0:\n",
    "        for ip in workingProxyList:\n",
    "            try:\n",
    "                randomIpNumber = random.randint(0,len(workingProxyList))\n",
    "                allPagesRequest.append(requests.get(url, timeout=15, proxies=workingProxyList[randomIpNumber]))\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        allPagesRequest.append(requests.get(url, timeout=10))\n",
    "    \n",
    "    if allpages > 5:\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemmer listen som indeholder alle requests data i rå format. Før filtering. \n",
    "def rawDataDump(allPagesRequest):\n",
    "    dumpList = []\n",
    "    tmp=\"\"\n",
    "    with tqdm(total=len(allPagesRequest)) as progress:\n",
    "        for x in (allPagesRequest):\n",
    "            progress.update(1)\n",
    "            dumpname = createFiles(x)\n",
    "            if dumpname in x.url:\n",
    "                if tmp == \"\":\n",
    "                    dumpList.append(x)\n",
    "                    tmp=dumpname\n",
    "                    with open(dataDir+dumpname+'.dump', 'wb') as output_file:\n",
    "                        pickle.dump(dumpList,output_file)\n",
    "                elif tmp==dumpname:\n",
    "                    dumpList.append(x)\n",
    "                    with open(dataDir+dumpname+'.dump', 'wb') as output_file:\n",
    "                        pickle.dump(dumpList,output_file)   \n",
    "                else:\n",
    "                    dumpList = []\n",
    "                    tmp=\"\"\n",
    "                    dumpList.append(x)\n",
    "                    with open(dataDir+dumpname+'.dump', 'wb') as output_file:\n",
    "                        pickle.dump(dumpList,output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd82723c6bcc43cc9a85700404468633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shop': 'https://dk.trustpilot.com/review/www.jyskmobelfabrik.dk', 'pages': '7'}\n",
      "{'shop': 'https://dk.trustpilot.com/review/www.cphbusiness.dk', 'pages': '2'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Websites du vil scrape fra på trustpilot.dk\n",
    "allShopsRequest = ['www.jyskmobelfabrik.dk', 'www.cphbusiness.dk']\n",
    "\n",
    "# Henter antal review sider for hver butik i listen og gemmer det som en liste af tuples.\n",
    "with tqdm(total=len(allShopsRequest)) as progress:\n",
    "    with ThreadPoolExecutor(2) as ex:\n",
    "        ex.map(multiThreadGetTotalPages, allShopsRequest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47774dc5431e4407bc0a0e05907343c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Henter alle siderne fra trustpilot og tilføjer dem i en liste.\n",
    "allpages=0    \n",
    "for x in totalpageprShop:\n",
    "    allpages = allpages+int(x['pages'])\n",
    "\n",
    "with tqdm(total=allpages) as progress:\n",
    "    for i in totalpageprShop:\n",
    "        digitlist = []\n",
    "        res = int(i['pages'])\n",
    "        for digit in range(res):\n",
    "            digitlist.append(digit)\n",
    "        with ThreadPoolExecutor(1) as ex:\n",
    "            ex.map(multiThreadDownloader,[i['shop']] * len(digitlist), digitlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5c6931c034485c8ec0761e3e12675d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Gemmer den rå data indeholdende alle requests.\n",
    "rawDataDump(allPagesRequest)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
